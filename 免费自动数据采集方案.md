# 🆓 完全免费的自动数据采集方案

## 📊 免费额度对比

| 服务 | 免费额度 | 限制 | 适用场景 |
|------|----------|------|----------|
| **Cloudflare Workers** | 100,000次请求/天 | 10ms CPU时间/请求 | ✅ 主力方案 |
| **Cloudflare Cron Triggers** | 无限制 | 最多3个cron | ✅ 定时任务 |
| **Cloudflare D1** | 5GB存储<br/>500万次读取/天<br/>10万次写入/天 | - | ✅ 边缘缓存 |
| **Supabase Free** | 500MB数据库<br/>2GB带宽/月<br/>5万次API请求/天 | - | ✅ 主数据库 |
| **GitHub Actions** | 2000分钟/月 | 每个workflow 6小时 | ✅ 备份方案 |

### 💰 成本分析

**每日数据采集需求**:
- 每5分钟同步一次 = 288次/天
- 每小时详细分析 = 24次/天
- 每天生成文章 = 1次/天
- **总计**: 约 **313次/天**

**实际使用**:
- Cloudflare Workers: 313次 (0.3% of 100k) ✅
- Cloudflare D1: ~3000次写入/天 (3% of 100k) ✅
- Supabase: ~300次写入/天 (0.6% of 50k) ✅

**结论**: **完全在免费额度内！** 🎉

---

## 🎯 推荐方案：Cloudflare Workers + Cron Triggers

### 架构图

```
┌─────────────────────────────────────────────────────┐
│         Cloudflare Cron Triggers (免费)              │
│          每5分钟自动触发一次                          │
└──────────────────┬──────────────────────────────────┘
                   ↓
┌─────────────────────────────────────────────────────┐
│       Cloudflare Workers (100k次/天免费)             │
│                                                     │
│  ┌────────────────────────────────────────────┐    │
│  │  /api/cron/sync-all                        │    │
│  │                                             │    │
│  │  1. 并行抓取8个API端点                      │    │
│  │  2. 数据清洗和转换                          │    │
│  │  3. 写入Cloudflare D1 (实时缓存)           │    │
│  │  4. 写入Supabase (历史数据)                │    │
│  └────────────────────────────────────────────┘    │
└──────────────────┬──────────┬───────────────────────┘
                   ↓          ↓
        ┌──────────────┐  ┌──────────────┐
        │Cloudflare D1 │  │  Supabase    │
        │   (边缘)     │  │  (中心)      │
        │              │  │              │
        │ 热数据       │  │ 完整历史     │
        │ <50ms响应    │  │ 复杂查询     │
        └──────────────┘  └──────────────┘
```

---

## 🔧 方案1: Cloudflare Workers Cron (推荐⭐)

### 优点
- ✅ **完全免费**: 无限制的cron触发
- ✅ **全球分发**: 在200+数据中心执行
- ✅ **高可靠**: Cloudflare的基础设施
- ✅ **零维护**: 无需管理服务器

### 配置步骤

#### Step 1: 更新 wrangler.toml

```toml
name = "alphaarena-live"
compatibility_date = "2024-09-23"
workers_dev = true
main = ".open-next/worker.js"
compatibility_flags = ["nodejs_compat"]

# Cloudflare D1 数据库绑定
[[d1_databases]]
binding = "DB"
database_name = "alphaarena-db"
database_id = "your-database-id-here"

# 环境变量
[vars]
NODE_ENV = "production"
NEXT_PUBLIC_APP_URL = "https://www.alphaarena-live.com"

# Cron触发器配置
[triggers]
crons = [
  "*/5 * * * *",    # 每5分钟：同步实时数据
  "0 * * * *",      # 每小时：同步详细分析
  "0 0 * * *"       # 每天UTC 00:00：生成文章
]
```

#### Step 2: 创建 Cloudflare D1 数据库

```bash
# 1. 创建D1数据库
wrangler d1 create alphaarena-db

# 2. 输出会显示database_id，复制到wrangler.toml中

# 3. 创建表结构
wrangler d1 execute alphaarena-db --file=./migrations/schema.sql
```

**schema.sql** (简化版，用于D1):

```sql
-- 排行榜缓存表
CREATE TABLE leaderboard_cache (
  model_id TEXT PRIMARY KEY,
  num_trades INTEGER,
  sharpe REAL,
  win_dollars REAL,
  num_losses INTEGER,
  lose_dollars REAL,
  return_pct REAL,
  equity REAL,
  num_wins INTEGER,
  rank INTEGER,
  cached_at INTEGER NOT NULL
);

-- 最新交易缓存（只保留最近100条）
CREATE TABLE recent_trades_cache (
  id TEXT PRIMARY KEY,
  model_id TEXT NOT NULL,
  symbol TEXT NOT NULL,
  side TEXT NOT NULL,
  entry_time INTEGER NOT NULL,
  exit_time INTEGER,
  realized_net_pnl REAL,
  trade_data TEXT NOT NULL, -- JSON字符串
  cached_at INTEGER NOT NULL
);

CREATE INDEX idx_recent_trades_time ON recent_trades_cache(entry_time DESC);
CREATE INDEX idx_recent_trades_model ON recent_trades_cache(model_id);

-- 每日统计缓存
CREATE TABLE daily_stats_cache (
  date TEXT PRIMARY KEY, -- YYYY-MM-DD
  total_trades INTEGER,
  total_pnl REAL,
  best_performer TEXT,
  cached_at INTEGER NOT NULL
);
```

#### Step 3: 创建完整的同步API

```typescript
// src/app/api/cron/sync-all/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

// 定义环境变量类型
interface Env {
  DB: D1Database
  CRON_SECRET: string
  NEXT_PUBLIC_SUPABASE_URL: string
  SUPABASE_SERVICE_ROLE_KEY: string
}

export const runtime = 'edge'
export const dynamic = 'force-dynamic'

// API端点列表
const API_ENDPOINTS = {
  leaderboard: 'https://nof1.ai/api/leaderboard',
  trades: 'https://nof1.ai/api/trades',
  analytics: 'https://nof1.ai/api/analytics',
  conversations: 'https://nof1.ai/api/conversations',
  accountTotals: 'https://nof1.ai/api/account-totals',
  sinceInception: 'https://nof1.ai/api/since-inception-values',
  cryptoPrices: 'https://nof1.ai/api/crypto-prices',
}

export async function GET(request: NextRequest) {
  const startTime = Date.now()

  // 1. 验证Cron Secret
  const authHeader = request.headers.get('authorization')
  const cronSecret = process.env.CRON_SECRET

  if (authHeader !== `Bearer ${cronSecret}`) {
    return NextResponse.json(
      { error: 'Unauthorized' },
      { status: 401 }
    )
  }

  try {
    // 2. 初始化数据库连接
    const supabase = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    )

    // 获取D1数据库（从Cloudflare环境）
    // 注意：在生产环境中，D1通过env绑定
    const db = (request as any).env?.DB

    // 3. 并行抓取所有API数据
    console.log('🚀 Starting data sync...')

    const results = await Promise.allSettled([
      syncLeaderboard(supabase, db),
      syncTrades(supabase, db),
      syncAnalytics(supabase, db),
      syncConversations(supabase),
    ])

    // 4. 统计结果
    const summary = {
      success: results.filter(r => r.status === 'fulfilled').length,
      failed: results.filter(r => r.status === 'rejected').length,
      results: results.map((r, i) => ({
        task: Object.keys(API_ENDPOINTS)[i],
        status: r.status,
        error: r.status === 'rejected' ? r.reason?.message : null,
      })),
      duration: Date.now() - startTime,
      timestamp: new Date().toISOString(),
    }

    console.log('✅ Sync completed:', summary)

    return NextResponse.json(summary)
  } catch (error: any) {
    console.error('❌ Sync failed:', error)

    return NextResponse.json(
      {
        error: 'Sync failed',
        message: error.message,
        duration: Date.now() - startTime,
      },
      { status: 500 }
    )
  }
}

// ========================================
// 同步排行榜数据
// ========================================
async function syncLeaderboard(supabase: any, db?: D1Database) {
  console.log('📊 Syncing leaderboard...')

  const response = await fetch(API_ENDPOINTS.leaderboard)
  if (!response.ok) throw new Error('Failed to fetch leaderboard')

  const data = await response.json()
  const timestamp = Date.now()

  // 保存到Supabase（历史快照）
  const snapshots = data.leaderboard.map((item: any, index: number) => ({
    snapshot_time: new Date(),
    model_id: item.id,
    rank: index + 1,
    num_trades: item.num_trades,
    sharpe: item.sharpe,
    win_dollars: item.win_dollars,
    num_losses: item.num_losses,
    lose_dollars: item.lose_dollars,
    return_pct: item.return_pct,
    equity: item.equity,
    num_wins: item.num_wins,
  }))

  const { error: supabaseError } = await supabase
    .from('leaderboard_snapshots')
    .insert(snapshots)

  if (supabaseError) {
    console.error('Supabase error:', supabaseError)
  }

  // 更新D1缓存（实时数据）
  if (db) {
    const stmt = db.prepare(`
      INSERT OR REPLACE INTO leaderboard_cache
      (model_id, num_trades, sharpe, win_dollars, num_losses, lose_dollars,
       return_pct, equity, num_wins, rank, cached_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `)

    await db.batch(
      data.leaderboard.map((item: any, index: number) =>
        stmt.bind(
          item.id,
          item.num_trades,
          item.sharpe,
          item.win_dollars,
          item.num_losses,
          item.lose_dollars,
          item.return_pct,
          item.equity,
          item.num_wins,
          index + 1,
          timestamp
        )
      )
    )
  }

  console.log(`✅ Leaderboard synced: ${data.leaderboard.length} models`)
  return { models: data.leaderboard.length }
}

// ========================================
// 同步交易数据
// ========================================
async function syncTrades(supabase: any, db?: D1Database) {
  console.log('💱 Syncing trades...')

  const response = await fetch(API_ENDPOINTS.trades)
  if (!response.ok) throw new Error('Failed to fetch trades')

  const data = await response.json()
  const timestamp = Date.now()

  // 获取已存在的trade_id
  const { data: existingTrades } = await supabase
    .from('trades')
    .select('trade_id')
    .order('entry_time', { ascending: false })
    .limit(1000)

  const existingIds = new Set(
    (existingTrades || []).map((t: any) => t.trade_id)
  )

  // 过滤出新交易
  const newTrades = data.trades.filter(
    (t: any) => !existingIds.has(t.trade_id)
  )

  if (newTrades.length > 0) {
    // 保存到Supabase
    const tradesData = newTrades.map((trade: any) => ({
      id: trade.id,
      trade_id: trade.trade_id,
      symbol: trade.symbol,
      side: trade.side,
      trade_type: trade.trade_type,
      model_id: trade.model_id,
      quantity: trade.quantity,
      leverage: trade.leverage,
      confidence: trade.confidence,
      entry_time: trade.entry_time,
      entry_human_time: trade.entry_human_time,
      entry_price: trade.entry_price,
      entry_sz: trade.entry_sz,
      entry_tid: trade.entry_tid,
      entry_oid: trade.entry_oid,
      entry_crossed: trade.entry_crossed,
      entry_commission_dollars: trade.entry_commission_dollars,
      entry_closed_pnl: trade.entry_closed_pnl,
      exit_time: trade.exit_time,
      exit_human_time: trade.exit_human_time,
      exit_price: trade.exit_price,
      exit_sz: trade.exit_sz,
      exit_tid: trade.exit_tid,
      exit_oid: trade.exit_oid,
      exit_crossed: trade.exit_crossed,
      exit_commission_dollars: trade.exit_commission_dollars,
      exit_closed_pnl: trade.exit_closed_pnl,
      realized_gross_pnl: trade.realized_gross_pnl,
      realized_net_pnl: trade.realized_net_pnl,
      total_commission_dollars: trade.total_commission_dollars,
    }))

    const { error } = await supabase.from('trades').insert(tradesData)

    if (error) {
      console.error('Trades insert error:', error)
    }

    console.log(`✅ Inserted ${newTrades.length} new trades`)
  } else {
    console.log('✅ No new trades')
  }

  // 更新D1缓存（最近100条）
  if (db) {
    // 清空旧缓存
    await db.prepare('DELETE FROM recent_trades_cache').run()

    // 插入最新100条
    const recent100 = data.trades.slice(0, 100)
    const stmt = db.prepare(`
      INSERT INTO recent_trades_cache
      (id, model_id, symbol, side, entry_time, exit_time, realized_net_pnl, trade_data, cached_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    `)

    await db.batch(
      recent100.map((trade: any) =>
        stmt.bind(
          trade.id,
          trade.model_id,
          trade.symbol,
          trade.side,
          trade.entry_time,
          trade.exit_time,
          trade.realized_net_pnl,
          JSON.stringify(trade),
          timestamp
        )
      )
    )
  }

  return { new_trades: newTrades.length, total: data.trades.length }
}

// ========================================
// 同步分析数据
// ========================================
async function syncAnalytics(supabase: any, db?: D1Database) {
  console.log('📈 Syncing analytics...')

  const response = await fetch(API_ENDPOINTS.analytics)
  if (!response.ok) throw new Error('Failed to fetch analytics')

  const data = await response.json()

  // 保存到Supabase
  const analyticsData = data.analytics.map((item: any) => ({
    snapshot_time: new Date(data.serverTime),
    model_id: item.model_id,
    // Fee & PnL
    std_net_pnl: item.fee_pnl_moves_breakdown_table?.std_net_pnl,
    total_fees_paid: item.fee_pnl_moves_breakdown_table?.total_fees_paid,
    overall_pnl_without_fees:
      item.fee_pnl_moves_breakdown_table?.overall_pnl_without_fees,
    total_fees_as_pct_of_pnl:
      item.fee_pnl_moves_breakdown_table?.total_fees_as_pct_of_pnl,
    overall_pnl_with_fees:
      item.fee_pnl_moves_breakdown_table?.overall_pnl_with_fees,
    avg_taker_fee: item.fee_pnl_moves_breakdown_table?.avg_taker_fee,
    std_gross_pnl: item.fee_pnl_moves_breakdown_table?.std_gross_pnl,
    avg_net_pnl: item.fee_pnl_moves_breakdown_table?.avg_net_pnl,
    biggest_net_loss: item.fee_pnl_moves_breakdown_table?.biggest_net_loss,
    biggest_net_gain: item.fee_pnl_moves_breakdown_table?.biggest_net_gain,
    avg_gross_pnl: item.fee_pnl_moves_breakdown_table?.avg_gross_pnl,
    std_taker_fee: item.fee_pnl_moves_breakdown_table?.std_taker_fee,
    // Winners & Losers
    win_rate: item.winners_losers_breakdown_table?.win_rate,
    avg_winners_net_pnl:
      item.winners_losers_breakdown_table?.avg_winners_net_pnl,
    avg_losers_net_pnl:
      item.winners_losers_breakdown_table?.avg_losers_net_pnl,
    avg_winners_notional:
      item.winners_losers_breakdown_table?.avg_winners_notional,
    avg_losers_notional:
      item.winners_losers_breakdown_table?.avg_losers_notional,
    avg_winners_holding_period:
      item.winners_losers_breakdown_table?.avg_winners_holding_period,
    avg_losers_holding_period:
      item.winners_losers_breakdown_table?.avg_losers_holding_period,
    // 原始数据
    raw_data: item,
  }))

  const { error } = await supabase
    .from('analytics_snapshots')
    .insert(analyticsData)

  if (error) {
    console.error('Analytics insert error:', error)
  }

  console.log(`✅ Analytics synced: ${data.analytics.length} models`)
  return { models: data.analytics.length }
}

// ========================================
// 同步对话数据
// ========================================
async function syncConversations(supabase: any) {
  console.log('💬 Syncing conversations...')

  const response = await fetch(API_ENDPOINTS.conversations)
  if (!response.ok) throw new Error('Failed to fetch conversations')

  const data = await response.json()

  // 获取已存在的对话ID
  const { data: existingConvos } = await supabase
    .from('ai_conversations')
    .select('id')
    .limit(1000)

  const existingIds = new Set((existingConvos || []).map((c: any) => c.id))

  // 过滤出新对话
  const newConvos = data.conversations?.filter(
    (c: any) => !existingIds.has(c.id)
  )

  if (newConvos && newConvos.length > 0) {
    const convosData = newConvos.map((convo: any) => ({
      id: convo.id,
      model_id: convo.model_id,
      conversation_time: new Date(convo.timestamp),
      user_prompt: convo.user_prompt,
      ai_response: convo.ai_response,
      decision_type: convo.decision_type,
      symbol: convo.symbol,
      action_taken: convo.action_taken,
      confidence: convo.confidence,
      raw_data: convo,
    }))

    const { error } = await supabase.from('ai_conversations').insert(convosData)

    if (error) {
      console.error('Conversations insert error:', error)
    }

    console.log(`✅ Inserted ${newConvos.length} new conversations`)
  } else {
    console.log('✅ No new conversations')
  }

  return { new_conversations: newConvos?.length || 0 }
}
```

#### Step 4: 部署

```bash
# 1. 构建和部署
pnpm workers:deploy

# 2. 设置环境变量（secrets）
echo "your_cron_secret" | wrangler secret put CRON_SECRET
echo "your_supabase_url" | wrangler secret put NEXT_PUBLIC_SUPABASE_URL
echo "your_service_role_key" | wrangler secret put SUPABASE_SERVICE_ROLE_KEY

# 3. 测试cron触发
curl -H "Authorization: Bearer your_cron_secret" \
  https://alphaarena-live.workers.dev/api/cron/sync-all
```

---

## 🔄 方案2: GitHub Actions (备份方案)

### 优点
- ✅ **完全免费**: 2000分钟/月
- ✅ **灵活配置**: 支持复杂工作流
- ✅ **独立运行**: 不依赖Workers

### 缺点
- ❌ 冷启动慢（30-60秒）
- ❌ 不能直接访问Cloudflare D1
- ❌ 只能写入Supabase

### 配置步骤

```yaml
# .github/workflows/sync-data.yml
name: Sync NOF1 Data

on:
  schedule:
    # 每5分钟运行一次
    - cron: '*/5 * * * *'
  workflow_dispatch: # 允许手动触发

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Sync leaderboard data
        run: pnpm sync-leaderboard
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Sync trades data
        run: pnpm sync-trades
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Sync analytics data
        run: pnpm sync-analytics
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
```

**对应的package.json脚本**:

```json
{
  "scripts": {
    "sync-leaderboard": "tsx scripts/sync/leaderboard.ts",
    "sync-trades": "tsx scripts/sync/trades.ts",
    "sync-analytics": "tsx scripts/sync/analytics.ts"
  }
}
```

---

## 📊 方案对比

| 特性 | Cloudflare Cron | GitHub Actions |
|------|----------------|----------------|
| **成本** | 完全免费 | 完全免费 |
| **执行速度** | 超快（<1秒） | 慢（30-60秒） |
| **可靠性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **D1访问** | ✅ 原生支持 | ❌ 不支持 |
| **Supabase访问** | ✅ 支持 | ✅ 支持 |
| **维护成本** | 极低 | 中等 |
| **推荐度** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |

---

## 🎯 最佳实践：混合方案

```
┌──────────────────────────────────────┐
│   Cloudflare Cron (主力)              │
│   - 每5分钟同步实时数据               │
│   - 写入D1 + Supabase                │
│   - 快速、可靠                       │
└──────────────────────────────────────┘
                 +
┌──────────────────────────────────────┐
│   GitHub Actions (备份)               │
│   - 每小时运行一次                    │
│   - 验证数据完整性                    │
│   - 生成每日报告                     │
└──────────────────────────────────────┘
```

---

## 🚀 快速开始（10分钟）

### 1. 创建Cloudflare D1数据库（2分钟）

```bash
cd C:\Users\Zero\trae\alphaarena

# 创建D1数据库
npx wrangler d1 create alphaarena-db

# 复制输出的database_id
```

### 2. 创建D1表结构（2分钟）

```bash
# 创建migrations目录
mkdir -p migrations

# 创建schema文件
# （使用上面提供的schema.sql内容）

# 执行创建表
npx wrangler d1 execute alphaarena-db --file=./migrations/schema.sql
```

### 3. 更新wrangler.toml（1分钟）

```toml
# 添加D1绑定
[[d1_databases]]
binding = "DB"
database_name = "alphaarena-db"
database_id = "你的database_id"

# 添加Cron触发器
[triggers]
crons = ["*/5 * * * *"]
```

### 4. 创建同步API（已提供完整代码）

复制上面的 `src/app/api/cron/sync-all/route.ts` 文件

### 5. 部署测试（5分钟）

```bash
# 部署到Cloudflare
pnpm workers:deploy

# 手动测试一次
curl -H "Authorization: Bearer your_cron_secret" \
  https://alphaarena-live.workers.dev/api/cron/sync-all
```

---

## 📈 监控和调试

### 查看Cron执行日志

```bash
# 实时查看Workers日志
npx wrangler tail

# 查看D1数据库内容
npx wrangler d1 execute alphaarena-db --command="SELECT * FROM leaderboard_cache"
```

### 验证数据同步

```sql
-- 在Supabase SQL Editor中查询
SELECT COUNT(*) FROM trades;
SELECT COUNT(*) FROM leaderboard_snapshots;
SELECT COUNT(*) FROM analytics_snapshots;

-- 查看最新数据
SELECT * FROM leaderboard_snapshots
ORDER BY snapshot_time DESC
LIMIT 10;
```

---

## 💡 成本节省技巧

1. **智能采样**：
   - 排行榜：每5分钟（高价值）
   - 交易记录：每5分钟（实时性重要）
   - 详细分析：每1小时（数据量大，变化慢）

2. **增量同步**：
   - 只同步新数据，避免重复写入
   - 使用`trade_id`去重

3. **批量操作**：
   - D1使用`db.batch()`批量插入
   - Supabase一次性插入多条记录

4. **缓存策略**：
   - D1存储最近100条交易（足够首页展示）
   - Supabase存储完整历史（供深度分析）

---

## ✅ 总结

**推荐方案**: **Cloudflare Workers Cron + D1 + Supabase**

**成本**: **完全免费** 🎉

**优势**:
- ✅ 全自动运行，无需干预
- ✅ 全球分发，响应极快
- ✅ 双数据库备份，数据安全
- ✅ 可扩展性强，支持商业化

**下一步**:
1. 创建D1数据库
2. 部署同步API
3. 测试数据采集
4. 监控运行状态

需要我帮你开始配置吗？ 🚀
