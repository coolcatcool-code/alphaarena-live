# ğŸ†“ å®Œå…¨å…è´¹çš„è‡ªåŠ¨æ•°æ®é‡‡é›†æ–¹æ¡ˆ

## ğŸ“Š å…è´¹é¢åº¦å¯¹æ¯”

| æœåŠ¡ | å…è´¹é¢åº¦ | é™åˆ¶ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|----------|
| **Cloudflare Workers** | 100,000æ¬¡è¯·æ±‚/å¤© | 10ms CPUæ—¶é—´/è¯·æ±‚ | âœ… ä¸»åŠ›æ–¹æ¡ˆ |
| **Cloudflare Cron Triggers** | æ— é™åˆ¶ | æœ€å¤š3ä¸ªcron | âœ… å®šæ—¶ä»»åŠ¡ |
| **Cloudflare D1** | 5GBå­˜å‚¨<br/>500ä¸‡æ¬¡è¯»å–/å¤©<br/>10ä¸‡æ¬¡å†™å…¥/å¤© | - | âœ… è¾¹ç¼˜ç¼“å­˜ |
| **Supabase Free** | 500MBæ•°æ®åº“<br/>2GBå¸¦å®½/æœˆ<br/>5ä¸‡æ¬¡APIè¯·æ±‚/å¤© | - | âœ… ä¸»æ•°æ®åº“ |
| **GitHub Actions** | 2000åˆ†é’Ÿ/æœˆ | æ¯ä¸ªworkflow 6å°æ—¶ | âœ… å¤‡ä»½æ–¹æ¡ˆ |

### ğŸ’° æˆæœ¬åˆ†æ

**æ¯æ—¥æ•°æ®é‡‡é›†éœ€æ±‚**:
- æ¯5åˆ†é’ŸåŒæ­¥ä¸€æ¬¡ = 288æ¬¡/å¤©
- æ¯å°æ—¶è¯¦ç»†åˆ†æ = 24æ¬¡/å¤©
- æ¯å¤©ç”Ÿæˆæ–‡ç«  = 1æ¬¡/å¤©
- **æ€»è®¡**: çº¦ **313æ¬¡/å¤©**

**å®é™…ä½¿ç”¨**:
- Cloudflare Workers: 313æ¬¡ (0.3% of 100k) âœ…
- Cloudflare D1: ~3000æ¬¡å†™å…¥/å¤© (3% of 100k) âœ…
- Supabase: ~300æ¬¡å†™å…¥/å¤© (0.6% of 50k) âœ…

**ç»“è®º**: **å®Œå…¨åœ¨å…è´¹é¢åº¦å†…ï¼** ğŸ‰

---

## ğŸ¯ æ¨èæ–¹æ¡ˆï¼šCloudflare Workers + Cron Triggers

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cloudflare Cron Triggers (å…è´¹)              â”‚
â”‚          æ¯5åˆ†é’Ÿè‡ªåŠ¨è§¦å‘ä¸€æ¬¡                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Cloudflare Workers (100kæ¬¡/å¤©å…è´¹)             â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  /api/cron/sync-all                        â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  1. å¹¶è¡ŒæŠ“å–8ä¸ªAPIç«¯ç‚¹                      â”‚    â”‚
â”‚  â”‚  2. æ•°æ®æ¸…æ´—å’Œè½¬æ¢                          â”‚    â”‚
â”‚  â”‚  3. å†™å…¥Cloudflare D1 (å®æ—¶ç¼“å­˜)           â”‚    â”‚
â”‚  â”‚  4. å†™å…¥Supabase (å†å²æ•°æ®)                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚Cloudflare D1 â”‚  â”‚  Supabase    â”‚
        â”‚   (è¾¹ç¼˜)     â”‚  â”‚  (ä¸­å¿ƒ)      â”‚
        â”‚              â”‚  â”‚              â”‚
        â”‚ çƒ­æ•°æ®       â”‚  â”‚ å®Œæ•´å†å²     â”‚
        â”‚ <50mså“åº”    â”‚  â”‚ å¤æ‚æŸ¥è¯¢     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æ–¹æ¡ˆ1: Cloudflare Workers Cron (æ¨èâ­)

### ä¼˜ç‚¹
- âœ… **å®Œå…¨å…è´¹**: æ— é™åˆ¶çš„cronè§¦å‘
- âœ… **å…¨çƒåˆ†å‘**: åœ¨200+æ•°æ®ä¸­å¿ƒæ‰§è¡Œ
- âœ… **é«˜å¯é **: Cloudflareçš„åŸºç¡€è®¾æ–½
- âœ… **é›¶ç»´æŠ¤**: æ— éœ€ç®¡ç†æœåŠ¡å™¨

### é…ç½®æ­¥éª¤

#### Step 1: æ›´æ–° wrangler.toml

```toml
name = "alphaarena-live"
compatibility_date = "2024-09-23"
workers_dev = true
main = ".open-next/worker.js"
compatibility_flags = ["nodejs_compat"]

# Cloudflare D1 æ•°æ®åº“ç»‘å®š
[[d1_databases]]
binding = "DB"
database_name = "alphaarena-db"
database_id = "your-database-id-here"

# ç¯å¢ƒå˜é‡
[vars]
NODE_ENV = "production"
NEXT_PUBLIC_APP_URL = "https://www.alphaarena-live.com"

# Cronè§¦å‘å™¨é…ç½®
[triggers]
crons = [
  "*/5 * * * *",    # æ¯5åˆ†é’Ÿï¼šåŒæ­¥å®æ—¶æ•°æ®
  "0 * * * *",      # æ¯å°æ—¶ï¼šåŒæ­¥è¯¦ç»†åˆ†æ
  "0 0 * * *"       # æ¯å¤©UTC 00:00ï¼šç”Ÿæˆæ–‡ç« 
]
```

#### Step 2: åˆ›å»º Cloudflare D1 æ•°æ®åº“

```bash
# 1. åˆ›å»ºD1æ•°æ®åº“
wrangler d1 create alphaarena-db

# 2. è¾“å‡ºä¼šæ˜¾ç¤ºdatabase_idï¼Œå¤åˆ¶åˆ°wrangler.tomlä¸­

# 3. åˆ›å»ºè¡¨ç»“æ„
wrangler d1 execute alphaarena-db --file=./migrations/schema.sql
```

**schema.sql** (ç®€åŒ–ç‰ˆï¼Œç”¨äºD1):

```sql
-- æ’è¡Œæ¦œç¼“å­˜è¡¨
CREATE TABLE leaderboard_cache (
  model_id TEXT PRIMARY KEY,
  num_trades INTEGER,
  sharpe REAL,
  win_dollars REAL,
  num_losses INTEGER,
  lose_dollars REAL,
  return_pct REAL,
  equity REAL,
  num_wins INTEGER,
  rank INTEGER,
  cached_at INTEGER NOT NULL
);

-- æœ€æ–°äº¤æ˜“ç¼“å­˜ï¼ˆåªä¿ç•™æœ€è¿‘100æ¡ï¼‰
CREATE TABLE recent_trades_cache (
  id TEXT PRIMARY KEY,
  model_id TEXT NOT NULL,
  symbol TEXT NOT NULL,
  side TEXT NOT NULL,
  entry_time INTEGER NOT NULL,
  exit_time INTEGER,
  realized_net_pnl REAL,
  trade_data TEXT NOT NULL, -- JSONå­—ç¬¦ä¸²
  cached_at INTEGER NOT NULL
);

CREATE INDEX idx_recent_trades_time ON recent_trades_cache(entry_time DESC);
CREATE INDEX idx_recent_trades_model ON recent_trades_cache(model_id);

-- æ¯æ—¥ç»Ÿè®¡ç¼“å­˜
CREATE TABLE daily_stats_cache (
  date TEXT PRIMARY KEY, -- YYYY-MM-DD
  total_trades INTEGER,
  total_pnl REAL,
  best_performer TEXT,
  cached_at INTEGER NOT NULL
);
```

#### Step 3: åˆ›å»ºå®Œæ•´çš„åŒæ­¥API

```typescript
// src/app/api/cron/sync-all/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

// å®šä¹‰ç¯å¢ƒå˜é‡ç±»å‹
interface Env {
  DB: D1Database
  CRON_SECRET: string
  NEXT_PUBLIC_SUPABASE_URL: string
  SUPABASE_SERVICE_ROLE_KEY: string
}

export const runtime = 'edge'
export const dynamic = 'force-dynamic'

// APIç«¯ç‚¹åˆ—è¡¨
const API_ENDPOINTS = {
  leaderboard: 'https://nof1.ai/api/leaderboard',
  trades: 'https://nof1.ai/api/trades',
  analytics: 'https://nof1.ai/api/analytics',
  conversations: 'https://nof1.ai/api/conversations',
  accountTotals: 'https://nof1.ai/api/account-totals',
  sinceInception: 'https://nof1.ai/api/since-inception-values',
  cryptoPrices: 'https://nof1.ai/api/crypto-prices',
}

export async function GET(request: NextRequest) {
  const startTime = Date.now()

  // 1. éªŒè¯Cron Secret
  const authHeader = request.headers.get('authorization')
  const cronSecret = process.env.CRON_SECRET

  if (authHeader !== `Bearer ${cronSecret}`) {
    return NextResponse.json(
      { error: 'Unauthorized' },
      { status: 401 }
    )
  }

  try {
    // 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    const supabase = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    )

    // è·å–D1æ•°æ®åº“ï¼ˆä»Cloudflareç¯å¢ƒï¼‰
    // æ³¨æ„ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒD1é€šè¿‡envç»‘å®š
    const db = (request as any).env?.DB

    // 3. å¹¶è¡ŒæŠ“å–æ‰€æœ‰APIæ•°æ®
    console.log('ğŸš€ Starting data sync...')

    const results = await Promise.allSettled([
      syncLeaderboard(supabase, db),
      syncTrades(supabase, db),
      syncAnalytics(supabase, db),
      syncConversations(supabase),
    ])

    // 4. ç»Ÿè®¡ç»“æœ
    const summary = {
      success: results.filter(r => r.status === 'fulfilled').length,
      failed: results.filter(r => r.status === 'rejected').length,
      results: results.map((r, i) => ({
        task: Object.keys(API_ENDPOINTS)[i],
        status: r.status,
        error: r.status === 'rejected' ? r.reason?.message : null,
      })),
      duration: Date.now() - startTime,
      timestamp: new Date().toISOString(),
    }

    console.log('âœ… Sync completed:', summary)

    return NextResponse.json(summary)
  } catch (error: any) {
    console.error('âŒ Sync failed:', error)

    return NextResponse.json(
      {
        error: 'Sync failed',
        message: error.message,
        duration: Date.now() - startTime,
      },
      { status: 500 }
    )
  }
}

// ========================================
// åŒæ­¥æ’è¡Œæ¦œæ•°æ®
// ========================================
async function syncLeaderboard(supabase: any, db?: D1Database) {
  console.log('ğŸ“Š Syncing leaderboard...')

  const response = await fetch(API_ENDPOINTS.leaderboard)
  if (!response.ok) throw new Error('Failed to fetch leaderboard')

  const data = await response.json()
  const timestamp = Date.now()

  // ä¿å­˜åˆ°Supabaseï¼ˆå†å²å¿«ç…§ï¼‰
  const snapshots = data.leaderboard.map((item: any, index: number) => ({
    snapshot_time: new Date(),
    model_id: item.id,
    rank: index + 1,
    num_trades: item.num_trades,
    sharpe: item.sharpe,
    win_dollars: item.win_dollars,
    num_losses: item.num_losses,
    lose_dollars: item.lose_dollars,
    return_pct: item.return_pct,
    equity: item.equity,
    num_wins: item.num_wins,
  }))

  const { error: supabaseError } = await supabase
    .from('leaderboard_snapshots')
    .insert(snapshots)

  if (supabaseError) {
    console.error('Supabase error:', supabaseError)
  }

  // æ›´æ–°D1ç¼“å­˜ï¼ˆå®æ—¶æ•°æ®ï¼‰
  if (db) {
    const stmt = db.prepare(`
      INSERT OR REPLACE INTO leaderboard_cache
      (model_id, num_trades, sharpe, win_dollars, num_losses, lose_dollars,
       return_pct, equity, num_wins, rank, cached_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `)

    await db.batch(
      data.leaderboard.map((item: any, index: number) =>
        stmt.bind(
          item.id,
          item.num_trades,
          item.sharpe,
          item.win_dollars,
          item.num_losses,
          item.lose_dollars,
          item.return_pct,
          item.equity,
          item.num_wins,
          index + 1,
          timestamp
        )
      )
    )
  }

  console.log(`âœ… Leaderboard synced: ${data.leaderboard.length} models`)
  return { models: data.leaderboard.length }
}

// ========================================
// åŒæ­¥äº¤æ˜“æ•°æ®
// ========================================
async function syncTrades(supabase: any, db?: D1Database) {
  console.log('ğŸ’± Syncing trades...')

  const response = await fetch(API_ENDPOINTS.trades)
  if (!response.ok) throw new Error('Failed to fetch trades')

  const data = await response.json()
  const timestamp = Date.now()

  // è·å–å·²å­˜åœ¨çš„trade_id
  const { data: existingTrades } = await supabase
    .from('trades')
    .select('trade_id')
    .order('entry_time', { ascending: false })
    .limit(1000)

  const existingIds = new Set(
    (existingTrades || []).map((t: any) => t.trade_id)
  )

  // è¿‡æ»¤å‡ºæ–°äº¤æ˜“
  const newTrades = data.trades.filter(
    (t: any) => !existingIds.has(t.trade_id)
  )

  if (newTrades.length > 0) {
    // ä¿å­˜åˆ°Supabase
    const tradesData = newTrades.map((trade: any) => ({
      id: trade.id,
      trade_id: trade.trade_id,
      symbol: trade.symbol,
      side: trade.side,
      trade_type: trade.trade_type,
      model_id: trade.model_id,
      quantity: trade.quantity,
      leverage: trade.leverage,
      confidence: trade.confidence,
      entry_time: trade.entry_time,
      entry_human_time: trade.entry_human_time,
      entry_price: trade.entry_price,
      entry_sz: trade.entry_sz,
      entry_tid: trade.entry_tid,
      entry_oid: trade.entry_oid,
      entry_crossed: trade.entry_crossed,
      entry_commission_dollars: trade.entry_commission_dollars,
      entry_closed_pnl: trade.entry_closed_pnl,
      exit_time: trade.exit_time,
      exit_human_time: trade.exit_human_time,
      exit_price: trade.exit_price,
      exit_sz: trade.exit_sz,
      exit_tid: trade.exit_tid,
      exit_oid: trade.exit_oid,
      exit_crossed: trade.exit_crossed,
      exit_commission_dollars: trade.exit_commission_dollars,
      exit_closed_pnl: trade.exit_closed_pnl,
      realized_gross_pnl: trade.realized_gross_pnl,
      realized_net_pnl: trade.realized_net_pnl,
      total_commission_dollars: trade.total_commission_dollars,
    }))

    const { error } = await supabase.from('trades').insert(tradesData)

    if (error) {
      console.error('Trades insert error:', error)
    }

    console.log(`âœ… Inserted ${newTrades.length} new trades`)
  } else {
    console.log('âœ… No new trades')
  }

  // æ›´æ–°D1ç¼“å­˜ï¼ˆæœ€è¿‘100æ¡ï¼‰
  if (db) {
    // æ¸…ç©ºæ—§ç¼“å­˜
    await db.prepare('DELETE FROM recent_trades_cache').run()

    // æ’å…¥æœ€æ–°100æ¡
    const recent100 = data.trades.slice(0, 100)
    const stmt = db.prepare(`
      INSERT INTO recent_trades_cache
      (id, model_id, symbol, side, entry_time, exit_time, realized_net_pnl, trade_data, cached_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    `)

    await db.batch(
      recent100.map((trade: any) =>
        stmt.bind(
          trade.id,
          trade.model_id,
          trade.symbol,
          trade.side,
          trade.entry_time,
          trade.exit_time,
          trade.realized_net_pnl,
          JSON.stringify(trade),
          timestamp
        )
      )
    )
  }

  return { new_trades: newTrades.length, total: data.trades.length }
}

// ========================================
// åŒæ­¥åˆ†ææ•°æ®
// ========================================
async function syncAnalytics(supabase: any, db?: D1Database) {
  console.log('ğŸ“ˆ Syncing analytics...')

  const response = await fetch(API_ENDPOINTS.analytics)
  if (!response.ok) throw new Error('Failed to fetch analytics')

  const data = await response.json()

  // ä¿å­˜åˆ°Supabase
  const analyticsData = data.analytics.map((item: any) => ({
    snapshot_time: new Date(data.serverTime),
    model_id: item.model_id,
    // Fee & PnL
    std_net_pnl: item.fee_pnl_moves_breakdown_table?.std_net_pnl,
    total_fees_paid: item.fee_pnl_moves_breakdown_table?.total_fees_paid,
    overall_pnl_without_fees:
      item.fee_pnl_moves_breakdown_table?.overall_pnl_without_fees,
    total_fees_as_pct_of_pnl:
      item.fee_pnl_moves_breakdown_table?.total_fees_as_pct_of_pnl,
    overall_pnl_with_fees:
      item.fee_pnl_moves_breakdown_table?.overall_pnl_with_fees,
    avg_taker_fee: item.fee_pnl_moves_breakdown_table?.avg_taker_fee,
    std_gross_pnl: item.fee_pnl_moves_breakdown_table?.std_gross_pnl,
    avg_net_pnl: item.fee_pnl_moves_breakdown_table?.avg_net_pnl,
    biggest_net_loss: item.fee_pnl_moves_breakdown_table?.biggest_net_loss,
    biggest_net_gain: item.fee_pnl_moves_breakdown_table?.biggest_net_gain,
    avg_gross_pnl: item.fee_pnl_moves_breakdown_table?.avg_gross_pnl,
    std_taker_fee: item.fee_pnl_moves_breakdown_table?.std_taker_fee,
    // Winners & Losers
    win_rate: item.winners_losers_breakdown_table?.win_rate,
    avg_winners_net_pnl:
      item.winners_losers_breakdown_table?.avg_winners_net_pnl,
    avg_losers_net_pnl:
      item.winners_losers_breakdown_table?.avg_losers_net_pnl,
    avg_winners_notional:
      item.winners_losers_breakdown_table?.avg_winners_notional,
    avg_losers_notional:
      item.winners_losers_breakdown_table?.avg_losers_notional,
    avg_winners_holding_period:
      item.winners_losers_breakdown_table?.avg_winners_holding_period,
    avg_losers_holding_period:
      item.winners_losers_breakdown_table?.avg_losers_holding_period,
    // åŸå§‹æ•°æ®
    raw_data: item,
  }))

  const { error } = await supabase
    .from('analytics_snapshots')
    .insert(analyticsData)

  if (error) {
    console.error('Analytics insert error:', error)
  }

  console.log(`âœ… Analytics synced: ${data.analytics.length} models`)
  return { models: data.analytics.length }
}

// ========================================
// åŒæ­¥å¯¹è¯æ•°æ®
// ========================================
async function syncConversations(supabase: any) {
  console.log('ğŸ’¬ Syncing conversations...')

  const response = await fetch(API_ENDPOINTS.conversations)
  if (!response.ok) throw new Error('Failed to fetch conversations')

  const data = await response.json()

  // è·å–å·²å­˜åœ¨çš„å¯¹è¯ID
  const { data: existingConvos } = await supabase
    .from('ai_conversations')
    .select('id')
    .limit(1000)

  const existingIds = new Set((existingConvos || []).map((c: any) => c.id))

  // è¿‡æ»¤å‡ºæ–°å¯¹è¯
  const newConvos = data.conversations?.filter(
    (c: any) => !existingIds.has(c.id)
  )

  if (newConvos && newConvos.length > 0) {
    const convosData = newConvos.map((convo: any) => ({
      id: convo.id,
      model_id: convo.model_id,
      conversation_time: new Date(convo.timestamp),
      user_prompt: convo.user_prompt,
      ai_response: convo.ai_response,
      decision_type: convo.decision_type,
      symbol: convo.symbol,
      action_taken: convo.action_taken,
      confidence: convo.confidence,
      raw_data: convo,
    }))

    const { error } = await supabase.from('ai_conversations').insert(convosData)

    if (error) {
      console.error('Conversations insert error:', error)
    }

    console.log(`âœ… Inserted ${newConvos.length} new conversations`)
  } else {
    console.log('âœ… No new conversations')
  }

  return { new_conversations: newConvos?.length || 0 }
}
```

#### Step 4: éƒ¨ç½²

```bash
# 1. æ„å»ºå’Œéƒ¨ç½²
pnpm workers:deploy

# 2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆsecretsï¼‰
echo "your_cron_secret" | wrangler secret put CRON_SECRET
echo "your_supabase_url" | wrangler secret put NEXT_PUBLIC_SUPABASE_URL
echo "your_service_role_key" | wrangler secret put SUPABASE_SERVICE_ROLE_KEY

# 3. æµ‹è¯•cronè§¦å‘
curl -H "Authorization: Bearer your_cron_secret" \
  https://alphaarena-live.workers.dev/api/cron/sync-all
```

---

## ğŸ”„ æ–¹æ¡ˆ2: GitHub Actions (å¤‡ä»½æ–¹æ¡ˆ)

### ä¼˜ç‚¹
- âœ… **å®Œå…¨å…è´¹**: 2000åˆ†é’Ÿ/æœˆ
- âœ… **çµæ´»é…ç½®**: æ”¯æŒå¤æ‚å·¥ä½œæµ
- âœ… **ç‹¬ç«‹è¿è¡Œ**: ä¸ä¾èµ–Workers

### ç¼ºç‚¹
- âŒ å†·å¯åŠ¨æ…¢ï¼ˆ30-60ç§’ï¼‰
- âŒ ä¸èƒ½ç›´æ¥è®¿é—®Cloudflare D1
- âŒ åªèƒ½å†™å…¥Supabase

### é…ç½®æ­¥éª¤

```yaml
# .github/workflows/sync-data.yml
name: Sync NOF1 Data

on:
  schedule:
    # æ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡
    - cron: '*/5 * * * *'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Sync leaderboard data
        run: pnpm sync-leaderboard
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Sync trades data
        run: pnpm sync-trades
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Sync analytics data
        run: pnpm sync-analytics
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
```

**å¯¹åº”çš„package.jsonè„šæœ¬**:

```json
{
  "scripts": {
    "sync-leaderboard": "tsx scripts/sync/leaderboard.ts",
    "sync-trades": "tsx scripts/sync/trades.ts",
    "sync-analytics": "tsx scripts/sync/analytics.ts"
  }
}
```

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | Cloudflare Cron | GitHub Actions |
|------|----------------|----------------|
| **æˆæœ¬** | å®Œå…¨å…è´¹ | å®Œå…¨å…è´¹ |
| **æ‰§è¡Œé€Ÿåº¦** | è¶…å¿«ï¼ˆ<1ç§’ï¼‰ | æ…¢ï¼ˆ30-60ç§’ï¼‰ |
| **å¯é æ€§** | â­â­â­â­â­ | â­â­â­â­ |
| **D1è®¿é—®** | âœ… åŸç”Ÿæ”¯æŒ | âŒ ä¸æ”¯æŒ |
| **Supabaseè®¿é—®** | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| **ç»´æŠ¤æˆæœ¬** | æä½ | ä¸­ç­‰ |
| **æ¨èåº¦** | â­â­â­â­â­ | â­â­â­ |

---

## ğŸ¯ æœ€ä½³å®è·µï¼šæ··åˆæ–¹æ¡ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloudflare Cron (ä¸»åŠ›)              â”‚
â”‚   - æ¯5åˆ†é’ŸåŒæ­¥å®æ—¶æ•°æ®               â”‚
â”‚   - å†™å…¥D1 + Supabase                â”‚
â”‚   - å¿«é€Ÿã€å¯é                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Actions (å¤‡ä»½)               â”‚
â”‚   - æ¯å°æ—¶è¿è¡Œä¸€æ¬¡                    â”‚
â”‚   - éªŒè¯æ•°æ®å®Œæ•´æ€§                    â”‚
â”‚   - ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ10åˆ†é’Ÿï¼‰

### 1. åˆ›å»ºCloudflare D1æ•°æ®åº“ï¼ˆ2åˆ†é’Ÿï¼‰

```bash
cd C:\Users\Zero\trae\alphaarena

# åˆ›å»ºD1æ•°æ®åº“
npx wrangler d1 create alphaarena-db

# å¤åˆ¶è¾“å‡ºçš„database_id
```

### 2. åˆ›å»ºD1è¡¨ç»“æ„ï¼ˆ2åˆ†é’Ÿï¼‰

```bash
# åˆ›å»ºmigrationsç›®å½•
mkdir -p migrations

# åˆ›å»ºschemaæ–‡ä»¶
# ï¼ˆä½¿ç”¨ä¸Šé¢æä¾›çš„schema.sqlå†…å®¹ï¼‰

# æ‰§è¡Œåˆ›å»ºè¡¨
npx wrangler d1 execute alphaarena-db --file=./migrations/schema.sql
```

### 3. æ›´æ–°wrangler.tomlï¼ˆ1åˆ†é’Ÿï¼‰

```toml
# æ·»åŠ D1ç»‘å®š
[[d1_databases]]
binding = "DB"
database_name = "alphaarena-db"
database_id = "ä½ çš„database_id"

# æ·»åŠ Cronè§¦å‘å™¨
[triggers]
crons = ["*/5 * * * *"]
```

### 4. åˆ›å»ºåŒæ­¥APIï¼ˆå·²æä¾›å®Œæ•´ä»£ç ï¼‰

å¤åˆ¶ä¸Šé¢çš„ `src/app/api/cron/sync-all/route.ts` æ–‡ä»¶

### 5. éƒ¨ç½²æµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# éƒ¨ç½²åˆ°Cloudflare
pnpm workers:deploy

# æ‰‹åŠ¨æµ‹è¯•ä¸€æ¬¡
curl -H "Authorization: Bearer your_cron_secret" \
  https://alphaarena-live.workers.dev/api/cron/sync-all
```

---

## ğŸ“ˆ ç›‘æ§å’Œè°ƒè¯•

### æŸ¥çœ‹Cronæ‰§è¡Œæ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹Workersæ—¥å¿—
npx wrangler tail

# æŸ¥çœ‹D1æ•°æ®åº“å†…å®¹
npx wrangler d1 execute alphaarena-db --command="SELECT * FROM leaderboard_cache"
```

### éªŒè¯æ•°æ®åŒæ­¥

```sql
-- åœ¨Supabase SQL Editorä¸­æŸ¥è¯¢
SELECT COUNT(*) FROM trades;
SELECT COUNT(*) FROM leaderboard_snapshots;
SELECT COUNT(*) FROM analytics_snapshots;

-- æŸ¥çœ‹æœ€æ–°æ•°æ®
SELECT * FROM leaderboard_snapshots
ORDER BY snapshot_time DESC
LIMIT 10;
```

---

## ğŸ’¡ æˆæœ¬èŠ‚çœæŠ€å·§

1. **æ™ºèƒ½é‡‡æ ·**ï¼š
   - æ’è¡Œæ¦œï¼šæ¯5åˆ†é’Ÿï¼ˆé«˜ä»·å€¼ï¼‰
   - äº¤æ˜“è®°å½•ï¼šæ¯5åˆ†é’Ÿï¼ˆå®æ—¶æ€§é‡è¦ï¼‰
   - è¯¦ç»†åˆ†æï¼šæ¯1å°æ—¶ï¼ˆæ•°æ®é‡å¤§ï¼Œå˜åŒ–æ…¢ï¼‰

2. **å¢é‡åŒæ­¥**ï¼š
   - åªåŒæ­¥æ–°æ•°æ®ï¼Œé¿å…é‡å¤å†™å…¥
   - ä½¿ç”¨`trade_id`å»é‡

3. **æ‰¹é‡æ“ä½œ**ï¼š
   - D1ä½¿ç”¨`db.batch()`æ‰¹é‡æ’å…¥
   - Supabaseä¸€æ¬¡æ€§æ’å…¥å¤šæ¡è®°å½•

4. **ç¼“å­˜ç­–ç•¥**ï¼š
   - D1å­˜å‚¨æœ€è¿‘100æ¡äº¤æ˜“ï¼ˆè¶³å¤Ÿé¦–é¡µå±•ç¤ºï¼‰
   - Supabaseå­˜å‚¨å®Œæ•´å†å²ï¼ˆä¾›æ·±åº¦åˆ†æï¼‰

---

## âœ… æ€»ç»“

**æ¨èæ–¹æ¡ˆ**: **Cloudflare Workers Cron + D1 + Supabase**

**æˆæœ¬**: **å®Œå…¨å…è´¹** ğŸ‰

**ä¼˜åŠ¿**:
- âœ… å…¨è‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€å¹²é¢„
- âœ… å…¨çƒåˆ†å‘ï¼Œå“åº”æå¿«
- âœ… åŒæ•°æ®åº“å¤‡ä»½ï¼Œæ•°æ®å®‰å…¨
- âœ… å¯æ‰©å±•æ€§å¼ºï¼Œæ”¯æŒå•†ä¸šåŒ–

**ä¸‹ä¸€æ­¥**:
1. åˆ›å»ºD1æ•°æ®åº“
2. éƒ¨ç½²åŒæ­¥API
3. æµ‹è¯•æ•°æ®é‡‡é›†
4. ç›‘æ§è¿è¡ŒçŠ¶æ€

éœ€è¦æˆ‘å¸®ä½ å¼€å§‹é…ç½®å—ï¼Ÿ ğŸš€
